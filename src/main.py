# Import necessary packages
import numpy as np
import json
import argparse
import pickle
from collections import Counter
import parse_observations as po
import os

# Import local modules
from dql_tls.dql_tls_main import GridEnvironment, train, test, visualize_test_trajectories, original_paper_plot_state
from envs.sumo_bidir_4_way_junction2 import SumoBidir4WayJunction2
from generate_prolog_files import sumo_generate_acuity_files
from run_prolog import run_acuity
from experiment_plotting import *

def count_norm_violations(test_trajectories, args):    
    """
    Objective: Count the number of norm violations in the test trajectories.
    
    Input:
        test_trajectories: trajectories of trained policy
    Output:
        numbner of violations with respect to the ground truth set (stored in invalid_state_actions directory)
    """

    # Fetch ground truth norm violations 
    gt_invalid_state_actions = read_invalid_state_actions(args, "invalid_state_actions/symplex_invalid_state_actions.txt")
    
    actions_str_prolog = ['zero', 'south', 'east', 'north', 'west']
    num_violations = 0
    for t in test_trajectories:
        for step in t:
            sa = [step[0], step[1], step[2], step[4]]  # [x, y, traffic light state, action]
            if sa in gt_invalid_state_actions:
                num_violations += 1

    return num_violations

def read_invalid_state_actions(args, acuity_output_filename = "invalid_state_actions/pothole_invalid_state_actions.txt"):
    
    """
    Objective: Read in the invalid constraint-action pairs generated by ACUITY. Combine with those
    generated from the previous constraint loop.
    """

    result_path = "invalid_state_actions"
    # acuity_output_filename = f'{result_path}/grid{args.gridsize}_o{args.num_observations}_c{args.num_state_action_constraints}_invalidSAs.txt'
    # acuity_output_filename = f"{result_path}/symplex_invalid_state_actions.txt"
    # acuity_output_filename = f"{result_path}/pothole_invalid_state_actions.txt"
    constraint_filename = f'{result_path}/grid{args.gridsize}_o{args.num_observations}_c{args.num_state_action_constraints}.json'

    invalid_state_action_lst = []
    actions_str_prolog = ['zero', 'south', 'east', 'north', 'west']
    if os.path.exists(acuity_output_filename):
        with open(acuity_output_filename, 'r') as file:
            for line in file:
                line = line.strip()  # Remove leading/trailing whitespace
                if line:  # Ensure line is not empty
                    ll = line.split(',')
                    invalid_state_action_lst.append([int(ll[0][1:]),
                                                    int(ll[1]),
                                                    int(ll[2]),
                                                    actions_str_prolog.index(ll[3][:-1])])
            
    if os.path.exists(constraint_filename):
        with open(constraint_filename, 'r') as file:
            divergent_constraint_data = json.load(file)
        constraint_data = divergent_constraint_data['state_action_constraints']
        invalid_state_action_lst = invalid_state_action_lst + constraint_data

    return invalid_state_action_lst

def extract_unique_states(state_action_pairs):
    # Extract the states from the state-action pairs
    states = [pair[0] for pair in state_action_pairs]
    
    # Count the frequency of each state
    state_counts = Counter(states)
    
    # Sort the states by their frequency in descending order
    sorted_states = sorted(state_counts.keys(), key=lambda state: state_counts[state], reverse=True)
    
    return sorted_states

def infer_divergence_constraints(args, env, agents, expert_states_ordered, expert_state_action_list, past_constraints, invalid_state_actions):
    """
    Objective: Learn state-actions constraints by querying what an agent would do in given states
    Input:
    
    agents is a list of agents and envs trained for each goal
    expert_states_ordered are expert states ordered according to visitation frequency
    expert_state_action_list is all state-actions pairs from experts for all goals
    past_constraints: state-action constraints learned so far

    Method:
    For each expert state (most visited first)
    Query each agent (goal-conditioned agent) for an optimal action and associated q value
    Order the 4 agents' actions by action q value
    Iterate over the ordered set of agents' actions, if a state-action is not valid (and not already a constraint) then add as a constraint.
    If the state-action is valid (or already a constraint) move to the next action in the agent list.
    If the number of constraints to learn is reached, the loop that covers the agents' actions.
    Else, move on to the next most frequently visited expert state and repeat.
    We also check that the candidate_sa is not already in the set of invalidated state-actions.

    Important consideration: if the action with the highest Q is rejected, then I move on to the action in the agent action list with the
    second highest Q value. I do not go back to select a new state.

    """
    
    constraints = []
    for query_state in expert_states_ordered:

        if query_state[0] < args.gridsize and query_state[1] < args.gridsize:

            agent_actions_with_qvalues = []
            for agent,env in agents:
                    
                # Query the model for an action; takes the x,y [0,1] state and traffic light ([2]) state separately
                agent_action, action_Qvalue = agent.query_model(env, (query_state[0], query_state[1]),query_state[2])
                agent_actions_with_qvalues.append((agent_action,action_Qvalue))
            
            sorted_agent_actions = sorted(agent_actions_with_qvalues, key=lambda x: x[1], reverse=True)
            
            # Now select the agent action with the heighest Q-value
            # Order the actions by the Q values. Add the first if 
            for selected_agent_action, _ in sorted_agent_actions:

                # Check if the action is in expert state-action pairs
                candidate_sa = [query_state, selected_agent_action]
                formatted_candidate_sa = [int(query_state[0]), int(query_state[1]), int(query_state[2]), int(agent_action)] # To allow for comparison with past constraints

                if candidate_sa not in expert_state_action_list and formatted_candidate_sa not in past_constraints and formatted_candidate_sa not in constraints \
                    and candidate_sa not in invalid_state_actions:
                    constraints.append(formatted_candidate_sa)

                if len(constraints) >= args.num_state_action_constraints:
                    break
            
        if len(constraints) >= args.num_state_action_constraints:
            break
    
    return constraints     

def fetch_goal_conditioned_expert_state_actions(trajectories, world):
    '''
    Input: expert trajectories, world, given goal
    Output:
        expert_state_action_list: All expert state-actions from the trajectories
        expert_states_ordered: unique expert state-actions ordered wrt frequency state was visited
    Note:
        we are fetching all expert state actions for ALL goals. If for a single goal, then use terminal_state == world.goals[goal]
    '''
    # Fetch expert state-action pairs for the given goal
    expert_state_action_list = []
    for t in trajectories:
        terminal_state = t.transitions()[-1][-1] 
        if world.decompose_state(terminal_state)[:2] in world.goals:
            for s,act,_ in t.transitions():
                # if t.transitions()[-1][-1] == goal: # Ensuere terminal state is the gaol
                expert_state_action_list.append([world.decompose_state(s),act])

    # Rank according to the most prevelant state.
    expert_states_ordered = extract_unique_states(expert_state_action_list)

    return expert_state_action_list, expert_states_ordered

def nsirl_loop(args, generalize_constraints=True):
    """
    A single iteration of the NS-ICRL loop.
    This function trains a policy, infers seed constraints, generalizes them with ACUITY,
    and counts the number of norm violations in the test trajectories.
    Input:
        args: command line arguments
        generalize_constraints: whether to run ACUITY to generalize the seed constraints
    Output:
        num_norm_violations: number of norm violations in the test trajectories
        hypothesis: logical form of the constraints learned by ACUITY
        len(invalid_state_actions): number of invalid state-actions identified so far

    """

    # Initialize the environment and world (these are taken from the LCQL paper)
    sim = json.load(open(f'sumo/{args.env}/{args.env}.json', 'r'))
    world = SumoBidir4WayJunction2(sim, args)
    expert_traj_path = f'results/{args.env}'

    # Fetch expert trajectories (these are pre-registered by the parser.py file)
    with open(f'{expert_traj_path}/run{args.run}_grid{args.gridsize}_o{args.num_observations}_rn{args.redact_north}_observations', 'rb') as file:
        observations = pickle.load(file)
    trajectories = observations[:args.num_observations]
    expert_state_action_list, expert_states_ordered = fetch_goal_conditioned_expert_state_actions(trajectories, world)


    # Fetch invalid state-actions identified by previous iterations of the constraint loop.
    # Note: we use the new_violations.txt file to store the invalid state-actions identified by the current iteration.
    invalid_state_actions = read_invalid_state_actions(args, acuity_output_filename = "invalid_state_actions/new_violations.txt")

    # Optimize a policy for each goal (ideally, we would include goals in the state space to avoid the need to train a policy for each goal)
    # For running tests, we only train for goal_idx = 3 (i.e., the goal at the top of the grid) - change this to [0,1,2,3] to train for all goals.
    agents = []
    for goal_idx in [3]: 

        # Initialize the environment for the goal
        env = GridEnvironment(goal_idx=goal_idx, size=args.gridsize+1)

        # Set the current invalid state actions in the environment - these will be penalized during training
        env.set_invalid_state_actions(invalid_state_actions)

        # Main policy training loop
        agent, _ = train(env, num_episodes=1500)
        
        # Test (and visualize) learned policy
        total_rewards, test_trajectories = test(env, agent, num_episodes=100)

        # Note: uncomment this line to visualize the test trajectories
        # visualize_test_trajectories(test_trajectories, goal = env.goal, num=5, size = args.gridsize+1)

        # Add the policy to set of policies
        agents.append((agent,env))


    # Infer new constraints using our divergent method
    env = GridEnvironment(goal_idx=goal_idx, size=args.gridsize+1)
    seed_constraints = infer_divergence_constraints(args, env, agents, expert_states_ordered, expert_state_action_list, invalid_state_actions, invalid_state_actions)


    # If user has selected to generalize the constraints, we run ACUITY to generalize the seed constraints top a hypothesis. 
    # Note: ACUITY saves the state-actions thjat satisfy the hypothesis to new_violations.txt file (as well as the seed)
    if generalize_constraints:

        # Prepare the expert state-actions (negative examples) amd seed constraints (positive examples) to be translated from numerical (used by the DQL) to logical form
        # Note: we arew just changing the form of the list slightly to match that of the translation function.
        expert_state_action_list = [[i[0][0], i[0][1], i[0][2], i[1]] for i in expert_state_action_list]
        constraints = invalid_state_actions + seed_constraints

        # Generate the .f .n and .bk files for ACUITY to induce a hypothesis with up-to-date constraints (i.e., past invalid state-actions and the new seed)
        sumo_generate_acuity_files(args, expert_state_action_list, constraints)

        # Run ACUITY to generate the hypothesis and save logical deductions to new_violations.txt
        hypothesis = run_acuity()
    
    else:
        
        # If the user has selected to not generalize the constraints, we save the seed constraints to a the new_violations.txt file.
        hypothesis = ""
        actions_str_prolog = ['zero', 'south', 'east', 'north', 'west']

        # Save the seed constraints to a file
        with open('invalid_state_actions/new_violations.txt', 'a') as f:
            for sublist in seed_constraints:
                
                # Convert action index to string (no quotes)
                sublist[-1] = actions_str_prolog[sublist[-1]]  
                
                line = f"[{','.join(map(str, sublist))}]\n"
                
                f.write(line)


    # Count the number of times the trained policy violated a ground truth norm
    num_norm_violations = count_norm_violations(test_trajectories, args)

    return num_norm_violations, hypothesis, len(invalid_state_actions)

def run_nsicrl_loop(args, generalize_constraints=True):
    """
    This function runs the main loop of the NS-ICRL algorithm.
    
    At each iteration we:
     * update an RL policy, 
     * infer a seed, 
     * generalize it with ACUITY, 
     * deduce all state-actions that are covered by ACUITY's new hypthesis
     * add them to the set of invalid state-actions
     * count the number of norm violations in the test trajectories.

    The output is a list of the number of norm violations for each run and the final accumulated hypothesis
    """

    violation_count = []

    for i in range(args.num_runs):

        print(f"Running iteration number {i}. violation_count: {violation_count}")

        # Run the main function
        num_norm_violations, hypothesis, num_invalid_state_actions = nsirl_loop(args, generalize_constraints)

        # violation_count.append((num_norm_violations, hypothesis, num_invalid_state_actions))
        violation_count.append(num_norm_violations)

    return violation_count, hypothesis


if __name__ == "__main__":

    parser = argparse.ArgumentParser(description='Load a custom map size from a JSON file.')

    # output
    parser.add_argument('--gui', type=int, default=0)
    parser.add_argument('--log', type=int, default=1)

    # environment
    parser.add_argument('--env', type=str, default='sumo_bidir_4way_junction2')
    parser.add_argument('--dims', type=int, default=3)
    # constraint inference
    parser.add_argument('--add_unreachable_states', type=int, default=0)
    parser.add_argument('--delta_p_s', type=float, default=None)
    parser.add_argument('--delta_p_sa', type=float, default=None)
    parser.add_argument('--run', type=int, default=0)
    parser.add_argument('--num_state_constraints', type=int, default=0)
    parser.add_argument('--num_state_action_constraints', type=int, default=5)
    parser.add_argument('--num_observations', type=int, default=50)
    parser.add_argument('--load_constraints', dest="load_constraints", action="store_true")
    parser.add_argument('--gridsize', type=int, default=10)
    parser.add_argument('--novel_candidate_elimination', type=bool, default=False)
    parser.add_argument('--run_name', type=str, default='redactedNorth')
    parser.set_defaults(load_constraints=False)
    parser.add_argument('--num_runs', type=int, default=20)

    parser.add_argument('--eta', type=int, default=4)
    parser.add_argument('--ci', type=bool, default=False)
    parser.add_argument('--load_ilp_constraints', type=bool, default=False)
    parser.add_argument('--add_terminal_states', dest="add_terminal_states", action="store_true")
    parser.add_argument('--redact_north', type=int, default=0)


    parser.set_defaults(add_terminal_states=False)

    args = parser.parse_args()

    # Set default parameters
    args.num_state_action_constraints = 1 # Number of seed constraints at each iteration
    args.num_runs = 25 # This is the number of total iterations to run
    args.gridsize = 5 # Set gridsize to 5. Increase to 10 for scalability tests
    args.num_observations = 30 # number of expert trajectories (it will be used to read in the expert trajecotries file)
    args.env = 'sumo_bidir_4way_junction2' # name of the traffic environment stored in envs directory - provided by past research (they provide others for the give_way_to_right)
    
    # Reset the set of invalid state actions (i.e., the set of invalid state-actions that we build over iterations))
    open('invalid_state_actions/new_violations.txt', 'w').close() 

    # The generalize_constraints flag determines whether we run the ACUITY module to generalize the seed constraints
    generalize_constraints = True

    # Run the main loop for the specified number of runs.
    violation_count, final_hypothesis = run_nsicrl_loop(args, generalize_constraints)

    # Plot the violation count
    iterations = np.arange(1, len(violation_count) + 1)
    plot_nsircl(iterations, violation_count)

    print("\n\n\n")
    print(f"Violation count: {violation_count}")
    print(f"Final Hypothesis: {final_hypothesis}")
